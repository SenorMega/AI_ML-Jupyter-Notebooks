{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Models and Vectorization Strategies for Text Classification\n",
    "\n",
    "This try-it focuses on weighing the positives and negatives of different estimators and vectorization strategies for a text classification problem.  In order to consider each of these components, you should make use of the `Pipeline` and `GridSearchCV` objects in scikitlearn to try different combinations of vectorizers with different estimators.  For each of these, you also want to use the `.cv_results_` to examine the time for the estimator to fit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data\n",
    "\n",
    "The dataset below is from [kaggle]() and contains a dataset named the \"ColBert Dataset\" created for this [paper](https://arxiv.org/pdf/2004.12765.pdf).  You are to use the text column to classify whether or not the text was humorous.  It is loaded and displayed below.\n",
    "\n",
    "**Note:** The original dataset contains 200K rows of data. It is best to try to use the full dtaset. If the original dataset is too large for your computer, please use the 'dataset-minimal.csv', which has been reduced to 100K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sspillane\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\sspillane\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sspillane\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import time \n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def stemmer(text):\n",
    "    stem = PorterStemmer()\n",
    "    return ' '.join([stem.stem(w) for w in word_tokenize(text)])\n",
    "\n",
    "def lemmatizer(text):\n",
    "    lemm = WordNetLemmatizer()\n",
    "    return ' '.join([lemm.lemmatize(w) for w in word_tokenize(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('humor', axis = 1)\n",
    "y = df['humor']\n",
    "\n",
    "# Stemmed data\n",
    "X_stemmed = X['text'].apply(stemmer)\n",
    "X_train_stemmed, X_test_stemmed, y_train_stemmed, y_test_stemmed = train_test_split(X_stemmed, y, random_state = 42)\n",
    "\n",
    "# Lemmatized data\n",
    "X_lemmed = X['text'].apply(lemmatizer)\n",
    "X_train_lemmed, X_test_lemmed, y_train_lemmed, y_test_lemmed = train_test_split(X_lemmed, y, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task\n",
    "\n",
    "\n",
    "**Text preprocessing:** As a pre-processing step, perform both `stemming` and `lemmatizing` to normalize your text before classifying. For each technique use both the `CountVectorize`r and `TfidifVectorizer` and use options for stop words and max features to prepare the text data for your estimator.\n",
    "\n",
    "**Classification:** Once you have prepared the text data with stemming lemmatizing techniques, consider `LogisticRegression`, `DecisionTreeClassifier`, and `MultinomialNB` as classification algorithms for the data. Compare their performance in terms of accuracy and speed.\n",
    "\n",
    "Share the results of your best classifier in the form of a table with the best version of each estimator, a dictionary of the best parameters and the best score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best_params</th>\n",
       "      <th>best_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistic</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bayes</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              best_params best_score\n",
       "model                               \n",
       "Logistic                            \n",
       "Decision Tree                       \n",
       "Bayes                               "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'model': ['Logistic', 'Decision Tree', 'Bayes'], \n",
    "             'best_params': ['', '', ''],\n",
    "             'best_score': ['', '', '']}).set_index('model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Vectorizer Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Reg\n",
    "log_vect_pipe = Pipeline([\n",
    "                          ('cvect', CountVectorizer()),\n",
    "                          ('lgr', LogisticRegression())\n",
    "                          ])\n",
    "log_vect_pipe_stemmed.fit(X_train_stemmed, y_train_stemmed)\n",
    "log_vect_test_acc_stemmed = log_vect_pipe_stemmed.score(X_test_stemmed, y_test_stemmed)\n",
    "\n",
    "log_vect_pipe_lemmed.fit(X_train_lemmed, y_train_lemmed)\n",
    "log_vect_test_acc = log_vect_pipe.score(X_test, y_test)\n",
    "\n",
    "\n",
    "# Decision Tree Classifier\n",
    "dt_vect_pipe = Pipeline([\n",
    "                          ('cvect', CountVectorizer()),\n",
    "                          ('lgr', DecisionTreeClassifier())\n",
    "                          ])\n",
    "dt_vect_pipe_stemmed.fit(X_train_stemmed, y_train_stemmed)\n",
    "dt_vect_test_acc = dt_vect_pipe.score(X_test, y_test)\n",
    "\n",
    "\n",
    "# Naive Bayes Classifier\n",
    "bayes_vect_pipe = Pipeline([\n",
    "                          ('cvect', CountVectorizer()),\n",
    "                          ('bayes', MultinomialNB())\n",
    "                          ])\n",
    "bayes_vect_pipe_stemmed.fit(X_train_stemmed, y_train_stemmed)\n",
    "bayes_vect_test_acc = bayes_vect_pipe.score(X_test, y_test)\n",
    "\n",
    "('bayes', MultinomialNB())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvect_params = {\n",
    "                'cvect__max_features': [100, 500, 1000, 2000],\n",
    "                'cvect__stop_words': ['english', None]\n",
    "                }\n",
    "\n",
    "# Count Vectorizer\n",
    "vect_grid = GridSearchCV(log_vect_pipe, param_grid=cvect_params)\n",
    "vect_grid.fit(X_train, y_train)\n",
    "test_acc = vect_grid.score(X_test, y_test)\n",
    "print(vect_grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF IDF Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Reg\n",
    "log_tfidf_pipe = Pipeline([\n",
    "                          ('tfidf', TfidfVectorizer()),\n",
    "                          ('lgr', LogisticRegression())\n",
    "                          ])\n",
    "log_tfidf_pipe.fit(X_train, y_train)\n",
    "log_tfidf_test_acc = log_tfidf_pipe.score(X_test, y_test)\n",
    "\n",
    "\n",
    "# Decision Tree Classifier\n",
    "dt_tfidf_pipe = Pipeline([\n",
    "                          ('tfidf', TfidfVectorizer()),\n",
    "                          ('lgr', DecisionTreeClassifier())\n",
    "                          ])\n",
    "dt_tfidf_pipe.fit(X_train, y_train)\n",
    "dt_tfidf_test_acc = dt_tfidf_pipe.score(X_test, y_test)\n",
    "\n",
    "\n",
    "# Naive Bayes Classifier\n",
    "bayes_tfidf_pipe = Pipeline([\n",
    "                          ('tfidf', TfidfVectorizer()),\n",
    "                          ('bayes', MultinomialNB())\n",
    "                          ])\n",
    "bayes_tfidf_pipe.fit(X_train, y_train)\n",
    "bayes_tfidf_test_acc = bayes_tfidf_pipe.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvect_params = {\n",
    "                'cvect__max_features': [100, 500, 1000, 2000],\n",
    "                'cvect__stop_words': ['english', None]\n",
    "                }\n",
    "\n",
    "# Count Vectorizer\n",
    "vect_grid = GridSearchCV(vect_pipe_1, param_grid=params)\n",
    "vect_grid.fit(X_train, y_train)\n",
    "test_acc = vect_grid.score(X_test, y_test)\n",
    "print(vect_grid.best_params_)\n",
    "\n",
    "\n",
    "tfidf_params = {\n",
    "                'tfidf__max_features': [100, 500, 1000, 2000],\n",
    "                'tfidf__stop_words': ['english', None]\n",
    "                }\n",
    "\n",
    "\n",
    "# Count Vectorizer\n",
    "tfidf_grid = GridSearchCV(vect_pipe_1, param_grid=params)\n",
    "tfidf_grid.fit(X_train, y_train)\n",
    "test_acc = tfidf_grid.score(X_test, y_test)\n",
    "print(tfidf_grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
