*Module 10: Title	
V#1	Problem Statement

Slide 1

So far we have built machine learning models for data sets that lacked a notion of time.

In this module we will learn methods that apply to data that present themselves in a sequential manner. Such data sets are very common. They are used to track econometric quantities such as GDP and inflation rates, in business for forecasting demand and commodity prices, as well as in science, medicine, finance, engineering, you name it.

The techniques that we will learn leverage the defining property of time series, which is that they are sequential. To analyze time series data, we will need new tools, such as the autocorrelation and partial autocorrelation functions, as well as new concepts such as trends, seasonality and stationarity.

We will learn two widely used models for representing time series data: the decomposition model, and the ARMA, or autoregressive, moving average model.

This is by no means a comprehensive account of the topic, but it will give you a good understanding of how to handle time series data, as well as two useful modeling tools for making forecasts.

Let’s get started. 

Slide 2
There are many problems involving time series data that we could address, but here we will focus on the most important of these problems: the “forecasting” problem.

The forecasting problem is stated as follows:

Given some historical data, try to predict – or forecast in the time series lingo – what will happen over some future time window.

This picture shows yearly data that rises and falls with a period of about a decade. Let’s assume that we are now in the year 2011, and we are tasked with predicting the future evolution of this time series over the next 10 years, from 2012 to 2021.

This is yearly data, however the methods that we will learn do not depend on the duration of the time step, only on the amount of data that we have. So it will make no difference whether we are making a 10 year prediction with 100 years of yearly data, or a 10 second prediction with 100 seconds of second-by-second data. Because these two situations involve the same number of data points, the results will be the same.

Slide 3
Our approach to the forecasting problem involves 4 steps.

First we will collect some up-to-the-moment historical data.

Then we will use that data to train a model.

Then we will use the model to make a forecast.

And finally,  once the forecast horizon has passed, we will score it. In other words, we will evaluate the performance of the model.

Slide 4
Ok. Let’s say we made our 10 year forecast back in 2011, and it is now 2021. So it’s time to assess how we did.

We need some notation. We’ll denote the value of the quantity at time t with y_sub_t.

The size of the historical data set is h, and the number of time steps being forecast is f.

We collect the historical data into an array y sub t minus h colon t, which we read y sub t minus h to t. In our case this is y 1987 to 2011

The bold font here indicates that this is a vector, not a scalar.
The 20-year forecast is denoted by a bold y with a hat on top and a subscript t to t+f. In our case this is yhat sub 2012 to 2021.

The actual data for that period is y sub 2012 to 2021.

And the prediction error e sub t is the difference between the prediction and the actual data for that period.

Slide 5
This error e sub t is an array, so we need to reduce it somehow to a single number.

There are two common ways to do this. Notice that simply taking the average of the elements in the array does not work, because positive and negative elements can cancel each other out, thus making a large error seem small.

However we can take the absolute values of the elements, and then take the mean of those. This is known as the mean absolute error, or MAE.

Another possibility is to use the root mean squared error, or RMSE. This is computed by taking the square root of the mean of the squared errors.  

Slide 6
Here’s an example.

In blue we have a time series consisting of 100 samples and we want to make a forecast of 10 steps into the future.

Our first task is to choose a forecasting model. A forecasting model is any equation or algorithm that generates the forecast from the historical data.

Here we see two very simple models. The first, shown in orange, is to say that the next 10 time step will be the same as the last measurement seen.
 
Well that’s pretty simplistic, and it also wastes all of the historical data, except for the most recent observation.

The second option, shown in green, is perhaps a bit better. It extrapolates with a linear regression taken over latest 5 data points. Again, this discards 95% of the historical data, but it is probably a better model.

To resolve which one of these two models is better, we must compare the scalar prediction errors that they produce. And we must do this continually as we move through time. It may be that one performs better in some cases and the other in other cases.

The techniques that we will learn in this module are quite a bit more sophisticated than these two. However it is always a good idea to compare the results you get with any model to the predictions from simple models such as these. You may be surprised by the results.

*V#2	Modeling

Slide 1

In this video we introduce new concepts from probability theory that will help us to analyze time series data.

Slide 2
In previous lectures we have described the data as having been generated by an unseen probability density function, and our task was to infer something about that PDF from the data.

Here again we take the same approach, except that now the random variables are organized in a sequential manner into a stochastic process.

A stochastic process is defined as an ordered sequence of random variables. Here we will denote them by capital Y-sub-t, with t varying from one to capital T.

Capital T is the length of the stochastic process, and it could be infinite. That is, the process could stretch back in to the deep past, with no discernible beginning.

Slide 3	
A note on the terminology.
The term “stochastic process” applies to the sequence of random variables
A time series is single sample from the stochastic process.
Here we see three different time series, all of which were produced from the same stochastic process.

Slide 4
Next we will learn about some important properties of stochastic processes. These are stationarity and independence.

Slide 5
A process is said to be stationary when its statistical properties – meaning its mean and variance and the correlations between different points in time – remain constant over time.

That is, if you focus on a time window of any size, then the statistics of the process within that window will be the same, no matter where you place the window in time.

In particular, if you choose a window of size one, this means that the marginal distributions at each time are identical. In mathematical notation, the distribution of the time series at some time t1 is equal to the distribution at any other time t2. The marginal distribution Y_t  is constant in time.

Slide 6
A process is independent when all of its constituent random variables are mutually independent.

It follows then that the joint probability of the entire stochastic process equals the product of the marginal probabilities across time.

An independent process can be difficult to forecast, because the values that we have seen in the past do not influence the current or future values. This is expressed mathematically by saying the distribution of Y at time t given all of the past information is equal to the distribution of Y_t without that information.

Slide 7
Stationarity and independence of stochastic processes are distinct from one another. A process can be stationary and not independent, or it can be independent and not stationary, or it can be neither, or both.

A process that is both stationary and independent is called iid, or independent and identically distributed.  

Slide 8
An important special case of an iid process is the Gaussian white noise process.
In this process every Y_sub_t is distributed according to a Gaussian distribution with zero mean and variance equal to one.

We will use Gaussian white noise processes later as generators of input noise for ARMA time series models

Slide 9
When trying to build a forecasting model, it is useful to estimate the correlations between pairs of random variables in the process. These correlations are usually organized into a matrix, called the autocorrelation matrix, “auto” because it is the correlation between two different time instants of the same process.

Let’s look closely at the structure of the autocorrelation matrix.

Along the diagonal we have all ones. These are the correlations of Y-sub_t with itself..

Then on the diagonals directly above and below that central diagonal we see autocorrelations between times that are one time-step apart .. Y1 and Y2, Y2 and Y3, etc.

Then another diagonal out are correlation between time instants that are two time-steps apart, and so on.

Slide 10
Recall that a stationary process is one for which the statistical properties of any window of data are the same, no matter where the window is placed in time.

So, considering a window of size 2 we realize that the correlations between Y1 and Y2 should equal the correlation for Y2 and Y3, etc. And thus we conclude that all of the values above and below the central diagonal must be the same. Here we call that value r1.

Similarly, with a window of size 3 we conclude that all of the entries that are two steps above or below the central diagonal should be equal, and we call them r2.

And so on as we move further from the central diagonal.

Thus, for stationary processes, the autocorrelation matrix has a very simple structure that is constant along diagonals.  

This observation motivates the use of the autocorrelation function, or ACF, instead of the autocorrelation matrix for stationary processes.

The ACF, shown here at a 45 degree angle, collects the values along the diagonals and plots them as a function of the lag, or the distance between points in time.

Slide 11
This is a typical autocorrelations plot.

The first entry, corresponding to a lag of zero, naturally equals one since it is the correlation of Y_sub_t with itself.  

For many processes the ACF will decay with lag, meaning that the data is more strongly correlated with recent measurements than with measurements further back in time.

However this is not always the case. We will see an example of this in an upcoming video.

Slide 12
Just as we can compute sample correlations between random variables from data, we can also compute the sample autocorrelation function for a stochastic process from a time series.

This is done by first computing the mean y_hat of the time series. Then computing the sample covariances c_sub_k for each lag value k with the formula shown here. An then dividing each c_k by c_sub_zero, so that the first sample autocorrelation rho_hat_sub_zero equals 1.

But don’t worry, you will not have to code this formul. In the next video we will learn how to compute this in Python using the statsmodels package.  

Let’s have a look at some example autocorrelation functions.

Slide 13
Here on the left we see an autocorrelation function for a white noise process. Because white noise is independent, there is no correlation with lags greater than zero.

The middle plot shows a sample of length 1000 from this white noise process.

We can feed this time series into the formula for the sample autocorrelation, and  produce the plot on the right for rho hat of k.

Notice a few things.

First, the values of rho and rho hat for k=0 are both exactly one. This is unsurprising since the formulas guarantee it.

Notice also that the sample autocorrelations for larger k are not identically zero, even though the true autocorrelations are zero. This is because these are merely estimates from the data we have. The blue band around these values is a threshold within which the sample autocorrelations can be regarded as negligible. This band gets narrower the more sample we have. As it is, this time series of length 1000 is long enough for us recognize that it was generated by a white noise process.  

Slide 14
In this example, the process on the right has a significant factor at a lag of 10. The time series that it produces has a vaguely periodic behavior with the period of about 10 time steps, and when we compute the sample autocorrelations we can see a factor that stands out significantly from the blue band at a lag of 10. Thus, from this sample of 100 points, we can infer that the structure of this stationary process has a strong correlations with lag=10.


_______________
*V#4	Decomposition

Slide 1

Here are some examples of time series.

On the top left we see a few recent years of the NASDAQ composite index. This data could be described as having a clear upward trend with a few abrupt variations.

The plot in the upper right show the number of spots observed on the surface of the sun. In contrast with the stock market, this data rises and falls periodically with no overall increasing or decreasing trend. We say that this data has periodicity or seasonality.

The bottom two plots are more irregular. On the left we see the rate of unemployment in the United States over the last 70 years. Like the sunsplots, this time series exhibits no clear upward or downward trend, however its variations do not occur periodically. Instead it exhibits occasional sharp rises followed by gradual decreases, and these cycles happen more or less randomly and have different duration.

The plot on the bottom right is a pure stationary process of the sort that we saw in the last video. Compared to the rest, it seems much noisier and less predictable.

And it is true, stationary processes are more difficult to predict than non-stationary processes, since they lack a trend or seasonality or cycles.

However, as we’ve seen, stationary processes are not necessarily white. They may still contain structure in the form of the autocorrelation function, which we can estimate and use to make short-term predictions.

In this video we will learn how to model long term behavior such as the trend, cycles and seasonality. And in a future video we will learn how to extract structure from a stationary processes using the ARMA framework.

Slide 2
Our approach will be to model the time series y as a composition of four terms: the trend, cycles, seasonality, and the residue.

The symbols in this formula are in bold font because they are vectors. y holds the historical data which we had previously denoted with a y sub_t minus h to t.

The trend is the long term behavior, such as the slow rise of the NASDAQ index.

The seasonality is the periodic behavior such as we saw in the sunspots dataset. Its key defining characteristic is that the period is constant and known to the modeler.

The cycles are variations that break with the trend, but do not happen with a set period.

Any behavior that does not fall into these three categories is considered as a residue or residual.

If the signal y is well characterized by this combination of a trend, cycles, a seasonality, then we’ll expect the residue to behave like a stationary process. That is, it will exhibit no structure at the time scales of cycles, seasons, and trends, however it may still contain hard-to-see structure at the level of a few time steps.  

Our next task is to extract these components.

We could think of many ways of doing this, but what I will show here is a basic and very common approach, and it is what is implemented in statsmodels package.

Slide 3
Our procedure has 3 steps.
First, we will compute the trend and cycles by smoothing the data. We will assume that the cycles are rare, and can therefore be lumped in with the trend.
Second, we’ll compute the seasonal component.
And third, we’ll check stationarity of the residue.

Slide 4
We will work with the sunspots data.
Imagine the year is 1984 and you have been tasked with predicting the next 15 years of sunspot activity. You are to base your prediction on observations of sunspots from 1900 to 1984.

Shown here in a lighter gray color are the sunspots that occurred after 1984, which we know in hindsight but will not use to build the model.

Slide 5
In the first step we extract the trend and cycles by smoothing the data. That is, by running y through a filter, which we call f.

f is an array of positive numbers that add up to 1. The length of f should be chosen to be similar but greater than  the period of the data. In this case the period is about 128 months, and so f has a length of about 129.

To compute t at some time we take a weighted average of nearby values of y, with weights given by f. Because the filter is longer than the period, it will remove the seasonal component completely, and leave only trend and cycles, assuming the cycles are longer than the seasonality and not too severe.

Slide 6
Here is the resulting trend in red. It looks pretty flat. So at this point we can make a decision to extrapolate it into the future with a horizontal line, as shown here in orange. Had we observed an increasing or decreasing trend, we might have used a different extrapolation technique. For example we may have used linear regression with linear, quadratic, or exponential basis functions.

Slide 7	
The next step in the process is to compute the seasonal component. To do this, we chop the historical time series into segments of length one period. Then we overlap those segments and take their average.

Like many topics in machine learning, time series analysis is a bit of an art, and decisions must be made at every step based on the data at hand.

At this point we notice that the period of the data changes slightly from one season to the next, and so we cannot define a single overall period.

We also notice that there is significant variation in the amplitude of the oscillations. For example, there was a large peak of around 250 sunspots in the late 1950s, while the peak in the 1970s was around 150.

However, throughout the historical data we observe that
a) the minimum number of sunspots in every season is very nearly zero, and
b) the peak is about twice the trend.

So how can we incorporate these observations into our model?  Here is a suggestion. Let’s replace our additive decomposition

yhat = t+s

with a multiplicative model
yhat = t*s

and scale the seasonal component so that it oscillates between 0 and 2. Then  yhat will oscillate between 0 and twice the trend, as desired.  

Slide 8
All right, let’s continue.

We have now divided the time series into 7 seasons, each lasting about a decade. Then we scaled each season so that its minimum and maximum values are 0 and 2. Those 7 lines are plotted here in gray.

Then we took the average of the 7 gray lines.

The resulting blue line is a bit too noisy to serve as our seasonal template. To deal with this, we will put the original data through a filter, just like we did to extract the trend, but this time the filter  will be much shorter so that it only smooths high frequency noise without affecting the seasonal behavior.

Slide 9
Ok, that looks a lot smoother.
But now we can see that there are some outliers in the data.
Let’s remove those and try again.

Slide 10
Ah, this looks much better. We were left with only three seasons, but the average now follows them quite closely.  

Now we can construct the seasonal signal by repeating the template we’ve just constructed.  

Slide 11
It looks like this. And if we now plug this into our formula.. yhat = t*s

Slide 12
this is what we get.

The red line is the trend. The blue line is the trend multiplied by the seasonal component, and it seems to follow the historical data pretty well

Slide 13
The final step in the process is to compute the residue of the model by subtracting yhat from y, and to check whether this signal is stationary.

This is done by checking the autocorrelation coefficients, as we will see in a future video.

If the residue is not stationary, then this might suggest that some long-term trend or seasonality may have remained unmodeled. In this case, it may be worthwhile to go back and revisit previous steps.

However if the residue is stationary, then there is not much more structure to be extracted using  decomposition techniques.

Slide 14
Let’s now use the model we have just built to make a prediction.

We do this simply by extending the seasonal component into the future, and in this case multiplying it by the extrapolation of the trend.

The result is shown here in green.

Looks pretty good to me.

Slide 15	
Now imagine that 17 years have gone by, and we have collected observations corresponding to the prediction we made back in 1984.

Just as we computed the residue as the historical data minus the model, we can now compute the prediction error as the observed data minus the forecast.

Here the prediction error is plotted in purple.

Slide 16
This table shows our two error metrics – mean absolute error and the root mean square error – applied to both the residue and the prediction error.

Both of these quantities represent estimates of the error in units of number of sunspots predicted.

And they both show something reasonable, which is that the residue is smaller than the prediction error.

To conclude the topic of time series decomposition, I’d like to stress that the essence of this technique is simply in the identification of trends, cycles and seasons as common structures in many real-world examples. However the details of how we decompose the time series into those parts vary widely depending on the application. And it is here that your own ingenuity and experience as a data modeler will come into play.

_______________
*V#6	ARMA

Slide 1

In the previous video we learned how to create a model of a time series that exhibits a trend and seasonality.

Having extracted those components we were left with a residue, which, in the best case scenario, would be stationary.

In this video we will introduce the ARMA family of models. These are designed to capture the time-invariant structure exhibited in stationary time series. Once this has been done, we will be left with a final residue of structureless and useless white noise.

Slide 2
Two of the most useful models for representing stationary processes are the “moving average” or MA process and the “autoregressive” or AR process. Here the q and the p are the orders of the moving average and autoregressive process respectively.

Slide 3
So, for example, MA(3) is a moving average process of order 3.

Both the MA and AR models regard the stationary process as the output y_t of a procedure that is fed with Gaussian white noise, which we denote by a_t.

Slide 4
For a moving average process of order q, the white noise is put through a filter with coefficients theta 1 through theta q. At each time t, the output y_t is computed by multiplying the theta coefficients by the past q values of the input noise, and then adding a_t. This is summarized in the formula for the moving average model.

Slide 5
The autoregressive process of order p is a little different. Instead of thetas, here we use the letter phi to represent the p coefficients of the model. And instead of feeding the white noise to the coefficients, we bring back the output y_t and use that. Hence the name auto-regressive. The procedure feeds back on itself.

The mathematical formula for the AR(p) process is shown here. Take a moment to observe and understand how each of these formulas correspond to their respective diagrams.

Slide 6
Here we see six samples of moving average and autoregressive processes of orders 2, 5, and 10. A zeroth order MA or AR process is just white noise, and the higher the order, the more structured the signal will become, generally speaking. However, just looking at these samples, it is difficult to guess whether the time series is MA or AR, let alone the order of the process. We are going to need better tools for analyzing stationary processes.

Slide 7
These plots show the autocorrelation functions for each of the processes whose samples we just saw.

Notice something interesting about the top row. The autocorrelation functions for moving average processes begin with a one, like all autocorrelation functions, and then is followed by as many non-zero entries as the order q of the process, and beyond that it is all zeros.

This observation gives us a direct method for identifying the order of a moving average process. Just look at the lag of the largest non-zero entry in the ACF.

The bottom row shows that this does not work for autoregressive processes. In this case, the zeroth entry is again 1, and after that the entries decay more or less exponentially, without every reaching absolute zero.

Slide 8
These plot show a different analytical tool called the partial autocorrelation function, or PACF. We will not go into the mathematical details of the PACF as we did with the ACF.

However inspecting the PACF we notice that the situation is now the reverse: the order p of the autoregressive processes shows up as the largest lag with a non-zero entry. So the AR(2) process has a non-zero PACF for lag 2 and zeros beyond that, and similarly for AR(5) and AR(10).

On the other hand, the partial autocorrelation functions for the moving average processes  are not very informative.

Ok, so it seems we now have two tools that will help us to determine the orders p and q of autoregressive and moving average processes. The problem is that we do not have these plots. All we have is the data, from which we can calculate sample autocorrelation and partial autocorrelation functions.

Let’s look at those.

Slide 9
The top row has sample autocorrrelation functions for the MA processes and the bottom are sample partial autocorrelation functions for the AR processes.

For MA and AR processes of orders 2 and 5, counting the number of non-zero coefficient before the plot dips into the blue region provides the correct answer.

However this is not the case for MA(10) and AR(10). If we were to guess the order of these processes on the basis of these plots, we would have guessed MA(6) and AR(5). For this reason, you rarely will encounter AR and MA models of such high order.

Slide 10
Having understood the autoregressive and moving average processes, we can now introduce the ARMA processes, which is a simple combination of these two elements.

Here on the bottom left we see the formula for the autoregressive process of order p. I've put the summation on the left hand side of the formula in order to gather all of the y's on the left and the a's on the right.

Then on the bottom right we have the formula for the moving average process of order q. Again, the y's are on the left and the a's on the right.

An ARMA process of orders p and q is obtained by taking the left hand side of AR(p) and equating it to the right hand side of MA(q).

ARMA(p,q) is a strictly larger class of models than both AR and MA, since we can get any AR model by setting q=0, and any MA model by setting p=0.

In fact, the ARMA family of models is very general as it can replicate any zero mean stationary process to a desired precision, by setting p and q large enough and carefully selecting the phi and theta coefficients.

Next we will learn how to do this.

Slide 11
The steps to modeling and forecasting time series data with ARMA are as follows.

First, one must check that the signal is stationary. Remember that ARMA only applies to stationary processes.

The second step is to use the sample autocorrelation and partial autocorrelation functions to select p and q, the orders of the AR and MA pieces respectively.

The third step is to find the values of the theta and phi coefficients of the model. The procedure for doing this is similar to other models we've seen in this course, such as linear regression, in that it is based on an optimization problem.

After training the coefficients, we now have a model and we can compute residuals. If the ARMA model has done its job, then the residuals will be white. 
The ARMA model can then be used to make a forecast.

Slide 12
We previously saw the autocorrelation function of the the residue from our model of the sunspots data, and we concluded that it was likely not stationary.

Let’s push ahead anyway and try to apply the ARMA model to this data.

Here we see both the autocorrelation and partial autocorrelations of the residue. The PACF decays very quickly, and suggests that we should consider an AR model of order 1.

Let’s try that.

Slide 13
To build the model we will use the ARIMA class from statsmodels.
The constructor for the ARIMA class takes the time series to be modeled, and the order of the model.

Here we are building a model of order p=1 and q=0. The zero between those two corresponds to the “I” in ARIMA.

Setting this to some number d will cause the model to first take d differences of the input data, in order to achieve stationarity. We’ve seen how differentiating a non-stationary time series can result in a stationary time series, so this is a common trick. However we will not use it here.

Having built the ARIMA object, we then call the fit() method on that object. This invokes the parameter training procedure. In our case we have only a single phi parameter to train.

Slide 14
The model can now be applied to the training data – that is, the residue from the decomposition model in this case. Confusingly perhaps, this is done with the predict method of the model. The result is seen here in orange, and compared to the original residue in blue. There is a pretty tight correspondence, but don’t be fooled. This is because the ARMA model is making only a one step prediction from each point in the original residue.

Slide 15
If we now subtract the arma prediction from the original residue, we get residue of the arma model, shown here in blue. Looking at the autocorrelation of that it is now quite close to white noise, and hence there is no further stationary structure to extract from the sunspots time series.

We’ve seen that ARMA significantly reduced the size of the model residues, but will it help with the ultimate goal of prediction?

The answer in this case is “no”. We should keep in mind that the reach of ARMA models into the future is commensurate with their order. Since the order of this model was p=1, we only expect it to make useful predictions a few steps into the future. This could be very helpful for predicting the number of sunspots a couple months from now, but does not help with our original task of forecasting 15 years into the future.

To summarize, we have seen two methods for forecasting time series data. The decomposition approach attempts to identify long term behaviors captured in trends and seasonality. The ARMA approach is very good for making short term predictions. This has been but an introduction to the large topic of time series forecasting, but I believe you now have the tools to investigate more advanced techniques on your own. Happy forecasting!


__________________

*BH-PCMLAI Berkeley Haas Professional Certificate in Machine Learning and Artificial Intelligence
Module 13: Logistic Regression
V#1	Motivation
Slide 1	

In the last module, you learned how to classify data using the K nearest neighbors approach. KNN is very intuitive, and it often does the job very well. However it has one significant downside, which is that, to classify a new item, you must find its nearest neighbors from amongst all of the training data. This can be quite expensive, both in terms of computation and memory.

This is very different from what we did with linear regression, where to decide the value of a new point, all we needed were the coefficients of the linear model, not the entire data set.

In this module we will start to learn about classification models that involve only a small number of coefficients, like linear regression. The simplest of these models, a direct analog to linear regression but for classification, is called logistic regression.

As we will see, naming it logistic regression as opposed to logistic classification makes sense, since the output of the model will be, not the discrete valued class to which the input belongs, but a continuous-valued probability that it belongs to this or that class.

Slide 2	
To demonstrate the technique, we will use the iris dataset. This dataset consists of measurement of the lengths and widths of the petals and sepals of 150 iris flowers. You can see here the data in a pandas dataframe. There are 5 columns: the sepal length, sepal width, petal length, petal width, and finally, the species of iris, as an integer 0, 1, or 2.
Slide 3
Here a 0 stands for the species setosa, a 1 is versicolor, and a 2 is a virginica type iris.

In all of these, the sepal, which is the outer protective part of the flower, is typically wider and longer than the interior petals, which are short and narrow.

Our goal will be to build a model that can classify iris flowers into setosa, versicolor, and virginica, given the sizes of their sepals and petals.

Slide 4	
Let's begin with a simpler problem. Let's see if we can distinguish between virginica and versicolor by their petal length alone. In this plot we see that virginica petals are typically larger than versicolor,

Slide 5	
and so we might imagine placing a threshold about here at 4.9 cm, and declaring that all flowers with petals larger than that number are classified as virginica, and all smaller are versicolor. Sounds good. Now all we need is a methodology for computing this threshold.

Slide 6	
Let's try using what we know, which is linear regression. That is, let's find a straight line that minimizes the sum of the squares of the vertical distances from the line to the data.
Then, wherever that line crosses the midpoint between versicolor and virginica, that is where we will place our threshold.

And here is the result. The linear regression line defines a threshold of 4.9 cm.

This worked well, but what if now we add an outlier flower to the dataset. If we repeat the computation we get a new threshold which is not too far from 4.9. But notice also how much the regression line was affected, from the green line without the outlier to the red line with the outlier.

In other words, the linear regression model is sensitive to outliers.

Logistic regression avoids the problem of sensitivity that we just witnessed with linear regression. Indeed, in this case the sensitivity of the decision threshold is one million times smaller with logistic regression than with linear regression. But it also has another significant advantage which is that it produces class membership probabilities instead of bare decision thresholds. We will see how these probabilities will be necessary to build classification models for more than two categories of objects.

___________
*V#2	One Feature, 2 Classes
Slide 1	

Imagine now that you had to guess whether an unseen flower was a versicolor or a virginica based only on its petal length. A reasonable way to go about this would be to go back to the dataset and find all flowers with similar petal lengths, and then decide by majority vote.

Here is an illustration of such a strategy.
 
First we grid the petal length space into some  number of bins. Then we count the number of each type of flower in each bin. The horizontal pink lines here represent the probability that a flower taken from that bin is virginica. And it is computed as follows:

The probability that the outcome Y equals virginica given that we are sampling from bin i equals the number of virginica flowers in bin i divided by the total number of flowers in bin i.

The probability that the flower is a versicolor is of course just 1 minus this quantity.

As we move from left to right, from smaller to larger petal lengths, we will expect the probability of virginica to rise from zero to one, just as we expect the probability of versicolor to decrease from 1 to 0.

The threshold petal length separating virginica and versicolor flowers should then be placed at the bin corresponding to equal parts of the two flower types. That is, where the probability equals 0.5,

Slide 2	
We are now going to formalize the example. Instead of the species of flower, we will refer to output class Y.

We will use class names 0 and 1, instead of versicolor and virginica.

And instead of petal length, we will consider some arbitrary feature called capital X.

To derive logistic regression we will begin by assuming that the marginal distribution of X – remember X is akin to the petal length – for each of the classes 0 and 1 are Gaussian.

In formal notation, this means that probability of X given Y=0 is normally distributed with mean mu 0 and variance v0 squared.  And the probability of X given Y=1 is normally distributed with mean mu1 and variance v1 squared

Furthermore, we will assume that the two variances are the same: v0 squared equals v1 squared, and we will denote both of them by v squared.

Let's now do the same thing we did before; let's look at a narrow bin and find the probability that the items within that bin are of type 1

Slide 3	
This probability will turn out to be very important, so we are going to give it its own symbol sigma. This might be a little confusing because we had previously used sigma for the standard deviation. But the reason will soon become clear.

Alright, just as before, this probability is the ratio of the number of items of class 1 divided by the total number of items. This equals the ratio of the blue shaded area divided by the sum of blue and orange.

if we now divide the numerator and denominator by the blue area, we see that sigma will depend only on the ratio of the two areas: blue divided by orange.

Let’s focus on that term.

Slide 4	
In fact, this ratio is an important quantity with its own name: the odds ratio.

Next we use the assumption that the marginal distributions are gaussian. We plug the formula for the gaussian distribution into the numerator and denominator of the odds ratio, keeping in mind that they have different means, mu0 and mu1

Because the variances were assumed to be the same, they cancel out, and we are left with a single exponential function of the difference between two squares: x minus mu1 squared, and x minus mu0 squared.

After a couple more lines of algebra, we arrive at our conclusion: that the odds ratio is equal to an exponential of a linear function of x.

Well that’s a mouthful, let’s take a closer look.

Slide 5	
The odds ratio equals an exponential: e to the minus z where z is a linear function of x with coefficients beta 0 and beta 1.

Furthermore, we can compute these betas from the parameters of the marginal distributions, mu0, mu1, and the common variance v squared.

Slide 6	
Ok, we can now use this expression for the odds ratio to complete our formula for sigma, and so we get that sigma of x equals 1 over 1 plus e to the minus z, where z equals beta 0 plus beta 1 times x.

This function is known as the sigmoid function, because of its similarity to a letter S. 
It is also known as the logistic function.

Slide 7	
Once we have found the particular logistic function that best fits our data, we can then use it to compute the threshold that separates the two classes.

Let’s call this threshold x bar.
As we said before, we choose the threshold such that the probabilities of each class are the same and equal to 0.5, so sigma of x bar must equal 0.5

Plugging this into the formula, we get that z bar must equal 0 since e to the 0 equals 1.

And so beta 0 + beta1 xbar must equal zero, and therefore the threshold xbar equals negative beta 0 over beta 1.

Slide 8	
Next we will address how to obtain the betas.

We already derived some formulas, but only by making some pretty broad assumption. We assumed not only that the marginals were Gaussian, but also that we know their mean and variance, and that the variances are equal.

We don’t know these quantities, but we can approximate them with the sample means and variances. Let’s call this the approximate solution.

In the real world this approximate solution will take us only so far. We will soon be extending this method to cases where no closed-form solution exists, and we will need a more powerful tool for computing the betas.

For this we will adopt the same approach as we have seen before with other models such as linear regression. That is, we will formulate logistic regression as an optimization problem.

Let’s do that.

Slide 9	
The cost function that we will use to chose the parameters beta0 and beta1 is the likelihood function.

Here is how it is defined.

The likelihood of a set of parameters – in this case beta 0 and beta 1 – is the probability that the dataset that we have observed could have been generated by the model implied by those parameters.

Let’s look at an example. Here we have a dataset with measurements of two classes, green and red. What is the likelihood of this candidate logistic function?

Well, this function implies the green and red marginal distributions that are well to the right of the data the we have. It’s not that it is impossible that this dataset could have arisen from the blue model, it’s just very unlikely.

So this logistic function  has a very low likelihood.  

Slide 10	
This pink model, on the other hand has a higher likelihood, and our objective will be to find the model with the highest likelihood, given the data.

Slide 11	
To quantify the likelihood of a candidate beta 0 and beta 1, we take the product of the probabilities of the data samples according to those parameters.

In this example, the probability of the red point of class 1 is sigma and of a green point of class 0 is 1 minus sigma. So we take the product of 1-sigma of xi for all green points, and multiply that by the product of sigma of xi for a red points.

If we assign a class label of zero to green and 1 to red, then we can express this as a single product of 1-sigma(xi) to the 1 minus yi times sigma(xi) to the yi.

Capital N in this formula is the number of samples.

Take a moment with this formula to make sure that you understand why it is true.

Slide 12	
The next step is to take the logarithm of both sides. This has the beneficial effect of converting the product into a sum, and it has no effect on the result.

Then using the properties of logarithms, and with a couple lines of algebra we get to the final objective function for our optimization

That is the sum over the data of one minus yi time log of 1 minus sigma + yi times log of sigma.

Slide 13	
This formula is known to information theorists as the cross entropy, or rather the negative of the cross entropy. So we will adopt that name. Instead of maximizing the likelihood, for classification problems we minimize the cross entropy. 

Slide 14
And here is the complete optimization problem for logistic regression. It reads: Find beta 0 and beta 1 that minimize the cross entropy, defined as the negative of this formula involving sigma, where sigma is defined as the logistic function. 

So to summarize, in this video we have derived a formula for the sigmoid or logistic function which will serve as a basis for simple classification problems.   We have posed the problem of selecting the parameters of the model as an optimization problem in which we minimize the cross entropy.

That’s it for basic logistic regression.  In the next couple of videos we will learn about extension of this technique to problems with 
A) more than one input  
B) more than two classes

See you then


_____________
*V#3		Many Features

Slide 1	

In the previous video we introduced the logistic regression model for classification problems.

Let’s recall that our original motivation for doing this was to classify iris flowers into three categories according to the length and width of their sepals and petals.

So far however we have only considered classification into two categories according to a single measurement.

In this video we will take the next step, and learn how to incorporate all four measurements into the model.

Slide 2	
We’ll begin with two features, X1 and X2.
The dataset is a collection of capital N samples, and each sample has values x1i and x2i, and a categorical value yi.
We can plot the data in the x1/x2 plane and color each sample according to its category yi.

Slide 3	
Now consider a new uncategorized sample x shown here as a green dot.
How shall we decide whether to classify x as orange or blue?
We can do the same thing as last time: that is, we can draw a small box around the new sample, count the number of blue and orange dots inside the box, and then decide by majority vote.

Slide 4	
To formalize this strategy, let’s again assume that the marginal distributions of the orange and blue classes are Gaussian and with equal covariance matrices capital Sigma, and furthermore that Sigma is diagonal.

Slide 5	
The same formula for the log odds from the one-dimensional case still applies here in the two dimensional case, except that now the terms are vector-valued.

The notation for the elements of x, mu1, mu2, and capital sigma are shown here.

Slide 6	
After some tedious algebra we arrive at this complicated formula for the log odds.
But the details are not the point here. The point is that the form of the log odds is identical to what we found in the single input case.
The log odds are again a linear combination of the inputs with an intercept term beta 0 and coefficients beta 1 and beta 2 for the two inputs
The fact that we made some very unrealistic assumptions to get to this formula is not very important, because we will not use this formula going forward. Again we will rely on the machinery of optimization to find the parameters.

But this result shows us a path forward. To add even more input features, all we need to do is add more linear terms to the log odds.

Slide 7	
And so we arrive at the general statement for two class logistic regression with capital M inputs.

Keep in mind that the random variable X is now multi-dimensional, and lower case x is a vector-valued sample.

The result is that the probability that a sample x is of class Y=1 equals the logistic function, sigma of x, where z is a linear combination of the M features with coefficients beta 0 through beta M.

And the betas are found, just as before, by minimizing the cross-entropy.

Slide 8	
Next we are going to address a common problem with logistic regression. We will illustrate this in the single feature case, but it shows up with multiple features as well.

Imagine you have a dataset where there is no overlap between the two classes. That is, there exists a threshold that exactly splits the data with perfect accuracy. What will the optimization problem do in this case?

Well, let’s consider two candidate solutions: sigma A in pink and sigma B in light blue color.

To evaluate the likelihood of each of these, we need to take the product of the probabilities of each data point, which are the heights of the vertical blue and pink lines in the figure.

And so we see that sigma B must have a higher likelihood than sigma A because in every case the vertical lines are longer.

But we could then take it one step further to a sigma C that is even steeper than sigma B, and then to sigma D, and so forth, ad infinitum.   

Slide 9	
In this case, there is no solution to the optimization problem

With each iteration of the optimizer, the coefficients beta0 and beta1 will continue to grow without bound, and the solution will converge toward an infinitely steep step function, like the one indicated here as sigma infinity

This bad situation was caused by the fact that the data is cleanly separable. And it is unfortunate, because separable data should be the easiest case to solve, and it shouldn’t cause the algorithm to explode.

To avoid this problem, we must add a regularization term to the cost function, to penalize large coefficient values.

Slide 10	
You’ve already learned about regularization, so I won’t go into it deeply here.
I will just mention that we can use either L1 regularization or L2 regularization to fix this problem.

Both of these amount to appending a term to the cross entropy cost function. In the L1 or LASSO case it is a sum of the absolute values of the coefficients, and in the L2 or Ridge regression case it is the sum of squares of the coefficients.

The effect in both cases is to limit the size of the coefficients.

One last note: the regularization parameter in scikitlearn is called “c”, not lambda, and it corresponds to the inverse of lambda. So to increase the strength of the regularization, you actually need to decrease “c”.

In the demo video I will demonstrate how to do this and how to use L1 regularization for feature selection.


________________
*V#4	Multiclass

Slide 1	

In the last video we developed the machinery for building logistic regression models that can use any number of features to distinguish between two classes of items.

However most of the interesting problems in the world involve more than two types of things.
For example we can think about

+ recognizing handwritten digits
+ predicting customer satisfaction
+ credit scoring

and many others in medicine, engineering, business, etc.

In this video we will complete the picture of logistic regression by extending our techniques to problems involving more than two classes

Slide 2	
We will cover 3 methods for adapting binary classification models to multi-class problems.

The first two: one-vs-rest and one-vs-one are generic and can be used with other classification models, aside from logistic regression.

One-vs-rest classification requires that the underlying binary classification model must return a continuous probability for the prediction, not simply a binary class assignment.

One-vs-one classification does not have this requirement. It can be used to extend any binary classifier to multi-class problems.

The last approach, multinomial regression, is a generalization of logistic regression to the multi-class case.

Slide 3	
To illustrate, we will use a 2-dimensional problem with 3 classes.

We will use a capital letter K for the number of classes. So in this case, K=3.

Slide 4	
The one-vs-rest or OVR strategy uses a binary classifier to build K separate models, each one pitting one class against all of the others.

Slide 5	
The first of these classifiers would be trained to distinguish class 1 from all other classes. Here we show the decision boundary for that model.  
But what we will actually use from the model is the continuous valued probability of class 1 as a function of x. If the binary classifier is logistic regression, then this will be a logistic function which we denote by sigma 1 of x.

Slide 6	
The one-vs-rest model consists of the K separate binary classifiers.

To predict the class of a new data point, shown here in green, we simply evaluate each of the binary classifiers and select the one that produces the largest value.

That is, we select the class that the point x is most likely to belong to according to the K binary classifiers.  

Slide 7	
Here we see the class regions that result from the one-vs-rest approach. Note that the boundaries are linear despite the fact that they were obtained by taking the maximum over sigmoid functions. The one-vs-rest extension of logistic regression therefore a linear classifier.

Slide 8	
The ovr approach made use of the probability estimate generated by the binary classifier.

However not all binary classifiers produce a probability estimate. The perceptron, for example, generates a decision boundary, but no probability estimate.

For this type of classifier, we can use the one-vs-one approach.
In one-vs-one, we run the binary classifier on every pair of classes. So if we have 3 classes we run 1-vs-2, 1-vs-3 and 2-vs-3.
The total number of executions of the binary classifier is K*(K-1)/2.
So for four classes we must construct 4*3/2 = 6 binary models
for 5 classes we must build 5*4/2 = 10 models, etc.

Slide 9	
We will focus for now on the simpler case of 3 classes.
The decision boundaries for the 3 binary classifiers are shown.
To classify a new data point, we evaluate all three binary classifiers and decide by majority vote.
In this case, the green sample is regarded as class 3 by two of the models, and as class 1 by the other. Hence it is predicted to be of class 3.

Slide 10	
These are the class regions created with one-vs-one.
The boundaries of these regions are boundaries of the binary classifiers. Thus we can conclude that, if the binary classifiers are linear, then the one-vs-one extension will also be linear.

Note though that there is a problem with this picture. The region in the middle is not shaded. This is because the voting in this region resulted in a 3-way tie amongst all of the classes, so the data could not be classified. This is a drawback of the one-vs-one approach.

Slide 11	
Here are a few pros and cons of these two strategies.
One-vs-rest has the advantage over one-vs-one that the number of models that need to be trained grows linearly with the number of classes, as opposed to quadratically with one-vs-one.

However the size of the training problems is smaller in one-vs-one by an factor of K-1.

Furthermore each of the model training runs in one-vs-one involves data from a single class at a time, and so the data is likely to be more balanced between positive and negative samples than with one-vs-rest.

As we saw, one-vs rest is only possible if the binary classifier returns a probability, whereas one-vs-one can always be used.

And finally one-vs-one has the drawback that it may not be able to classify some data points for which there is no unique winner in the voting process, whereas one-vs-rest does not have this problem.

Slide 12	
Next we will talk about multinomial regression, which is our most general form of logistic regression.

Multinomial regression is similar to one-vs-rest in that we will calculate a continuous probability for each class, and it is similar to one-vs-one in that each of the models will be trained by comparing one class to just one other.

The derivation begins by specifying a reference class to which all other classes will be compared. We will use class K as the reference, however the result would be the same with any other selection.

Next we build K-1 logistic regression models by comparing each of classes 1 through K-1 to the reference class K.

Let’s begin with the model 1 vs. K.
The trained coefficients beta 1,0 through beta 1,m combine linearly with the features to produce the log odds, which is the logarithm of the probability of class 1 over the probability of class K.

To make the notation simpler, let’s define a vector beta 1 of coefficients for this model and denote the linear combination as negative beta 1 dot x.

Then we can do the same for classes two through K-1.

Slide 13	
From each of these we can express the probability of any class kappa as the probability of class K times the exponential of negative beta dot x.

Here kappa is any class 1 to K-1

Slide 14	
Next we observe that, because the class probabilities must add up to 1, the probability of class K equals 1 minus the sum of probabilities from 1 to K-1.

We can now use the expression for the probability of class kappa that we just derived and obtain:

Slide 15	
that the probability of class K equals 1 over 1 + the sum of the odds of all other classes.
Using this expression we get the probability of the other classes 1 through K-1, as the odds of that class divided by the same denominator.

These are the final formulas for multinomial logistic regression. They may seem complicated at first, but comparing them to the original logistic regression you will see that they are actually a very reasonable generalization.

Furthermore they have solved the problem of multi-class classification in a much more elegant way for logistic regression than was achieved with the generic approaches of one-versus-rest and one-vs-one, since we have obtained explicit formulas for the probabilities with only K-1 binary models.

That’s it for multi-class regression.

In the next video we will learn how to do all of this in code.

	
	

	
	
	

